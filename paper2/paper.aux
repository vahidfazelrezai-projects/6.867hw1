\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Gradient Descent}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Batch Gradient Descent}{}}
\newlabel{fig:gd}{{2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The effects of $\lambda $ (left), $\delta $ (middle), and $\mathbf  {x}_0$ (right) on gradient descent.}}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Stochastic Gradient Descent}{}}
\newlabel{fig:gd_steps}{{2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Magnitude of the gradient vs. number of iterations. Stochastic gradient descent (left) is much noisier than batch gradient descent (right).}}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Effect of Finite Gradient}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Function Basis}{}}
\newlabel{fig:cfd}{{2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces The change in central finite difference as the value of $h$ increases for Gaussian (left) and quadratic bowl (right). The real values are $-3.1704 \cdot 10^{-7}$ and $-280$, respectively.}}{}}
\newlabel{eq:predictor}{{1}{}}
\newlabel{eq:problem}{{2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Linear Regression}{}}
\newlabel{eq:sse}{{3}{}}
\newlabel{eq:linearclosed}{{4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Polynomials with Closed Form}{}}
\newlabel{fig:poly}{{4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Training data (green), generating curve (red), and predicted curve (blue) for $n = 0, 1, 3, 10$.}}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Polynomials with Gradient Descent}{}}
\newlabel{fig:gd_m}{{4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces The results for GD and SGD for $M = 3, 5, 7, 10$ (in that order).}}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Cosines with Closed Form and Gradient Descent}{}}
\newlabel{fig:cos}{{4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Training data (green), generating curve (red), and predicted curve (blue) for $n=2, 5, 8$.}}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Ridge Regression}{}}
\newlabel{eq:ridgeobjective}{{5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Continuation of Simple Example}{}}
\newlabel{fig:ridge}{{5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Training data (green), generating curve (red), and predicted curve (blue) for $\lambda =0.0001, 0.1$ and $n = 3, 10$.}}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Training and Validation}{}}
\newlabel{fig:validationtable}{{5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces SSE values on validation set after training with various $n$ (rows) and $\lambda $ (columns). Left: training on set A. Right: training on set B. Best values bolded.}}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}LASSO}{}}
\newlabel{eq:ridgeclosed}{{6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Sparsity in LASSO versus Ridge Regression}{}}
\newlabel{fig:everything}{{6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Comparison of various prediction curves plotted with data points.}}{}}
