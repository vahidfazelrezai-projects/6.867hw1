\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Overview}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Gradient Descent}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Batch Gradient Descent}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces As the step size increases, the algorithm converges quicker.}}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces As $\lambda $ and $\delta $ decrease, error decreases.}}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces As $\mathbf  {x}_0$ appraoches $(10, 10)$, the rate of convergence increases.}}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Stochastic Gradient Descent}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Effect of Finite Gradient}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces As $h$ increases, the accuracy of the central finite difference decreases. The actual gradient value is $-3.1704 \cdot 10^{-7}$.}}{3}}
